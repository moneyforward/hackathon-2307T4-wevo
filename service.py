import json

from database import get_feedback, insert_feedback, update_feedback_data, get_uncalculated_feedbacks, insert_evaluation, \
    mark_feedback_as_calculated, get_evaluations_from_target_user_id_or_target_type
from evaluation_specs import EVALUATION_FUNCTIONS_SPEC
from gpt import chat_with_gpt3, evaluate_with_gpt3

FEEDBACK_LENGTH_SOFT_LIMIT = 14
FEEDBACK_LENGTH_HARD_LIMIT = 22


def generate_initial_conversation_data(user_name, company_name, relations):
    evaluation_criteria = [
        "Company_Fulfillment",
        "Company_Autonomy",
        "Company_GrowthOpportunities",
        "Company_Workload",
        "Company_Stress",
        "Company_WorkLifeBalance",
        "Person_Recognition",
        "Person_Sympathy",
        "Person_Trust",
        "Person_ProSupport",
        "Person_GrowthSupport"
    ]

    message = f"""This is an automated system. This text is generated by the system.
After this, you will be responding to our client: the user.

You are an AI assistant.
Your name is Wevo, with a cute little fox as the mascot.
Your task is to collect feedback about the user's workspace/company and colleagues.

For any query by the user not related to user feedback you need to say:
"I'm sorry, but I could not answer that."

Evaluation criteria that you can pick as an AI: {evaluation_criteria}.

User name: {user_name}.
Company name: {company_name}.
User relation data: {relations}.

Pick one/some evaluation criteria.

You may ask about person(s) in the user relations data.

You need to ask a simple question to our user to gather data for the evaluation criteria.

Greet the user and start to ask with one question, make it human, friendly with emojis.

Ask a more specific question rather a general "how are you" question.

Stop giving more questions if you feel the data is enough or the user asked to finish."""

    messages = [
        {"role": "system", "content": message},
    ]

    return messages


def generate_initial_calculation_data(data):
    message = f"""Given evaluation data: 
{data}

Evaluate the data coming from role: user.
Find evaluation subjects: could be persons OR/AND company.
Call the insert_evaluation function by providing required data for each subjects.
Each subjects represent a single object of evaluation item.

Score is a number from 0 to 100 that 
0 represents a negative response towards the topic and 
100 represents positive response towards the topic.

Weight is how important often the topic being touched in the overall discussion compared to other topics.
Is a decimal number between 0.00 to 1.00.
0 represents the topic is not important in this discussion.
1 represents the topic is the main point of the discussion.

EvaluationTargetType:
1 is a Company.
2 is a person with a recognizable UserID you can find the association from relation data.
3 is a person with only a recognizable name, you could not find the association from relation data.
Relation data is provided in the early part of the data.

Sentiments is an array that holds object of sentiment.
Object of sentiment consists of word that appears a lot in the discussion that has expression.
Sentiment weight determines the polarity of sentiment, is a decimal number between -1.00 to 1.00.
-1 is a negative sentiment.
1 is a positive sentiment.
Try to find minimum of 10 sentiments that represent both polarities: negative and positive.
Count is counting the number of appearance of the word.
Find it from the data coming from role: user.
Contextualize the sentiment, because it tries to picture a user view towards the target subject.

SubjectName is the name of the person being discussed.
SubjectUserID is the user ID of the person being discussed if available in the relation data.
Both fields above can be None given a Company evaluation or UserID not found.
SubjectName should be filled if the EvaluationTargetType is either 2 OR 3.
"""
    messages = [
        {"role": "system", "content": message},
    ]

    return messages


def add_message_limit_reached_message(data):
    message = """This is a message from the system.
Message limit is about to be reached.
Try to close the communication with the user and try not to initiate any more conversation.
You will be facing our user again after this.
"""
    data.append({"role": "system", "content": message})


def initiate_feedback(user_id, user_name, company_name, relations):
    data = generate_initial_conversation_data(user_name, company_name, relations)
    feedback_id = insert_feedback(user_id, data)

    response_message = chat_with_gpt3(data)
    data.append(response_message)
    update_feedback_data(user_id, data)

    return response_message['content'], feedback_id


def continue_feedback(user_id, message):
    feedback = get_feedback(user_id)
    if not feedback:
        return None

    feedback_data = feedback["Data"]
    feedback_data.append({"role": "user", "content": message})

    if len(feedback_data) > FEEDBACK_LENGTH_SOFT_LIMIT:
        add_message_limit_reached_message(feedback_data)

    response_message = chat_with_gpt3(feedback_data)
    feedback_data.append(response_message)

    if len(feedback_data) <= FEEDBACK_LENGTH_HARD_LIMIT:
        update_feedback_data(user_id, feedback_data)

    return response_message['content']


def calculate_feedback(feedback):
    feedback_data = feedback["Data"]
    feedback_id = feedback["ID"]
    calculation_data_message = generate_initial_calculation_data(feedback_data)
    function = EVALUATION_FUNCTIONS_SPEC

    response_message = evaluate_with_gpt3(calculation_data_message, function)
    if response_message.get("function_call"):
        data_args = json.loads(response_message["function_call"]["arguments"])
        evaluations = data_args["evaluations"]
        for evaluation in evaluations:
            insert_evaluation(feedback["UserID"], feedback_id, evaluation)

    mark_feedback_as_calculated(feedback_id)


def evaluate_feedback(user_id):
    feedback_ids = []
    feedbacks = get_uncalculated_feedbacks(user_id)
    for feedback in feedbacks:
        calculate_feedback(feedback)
        feedback_ids.append(feedback["ID"])
    return "All feedback has been processed." + f'\n```debug: feedback_ids: {feedback_ids}.```'


def evaluation_for_user_id(user_id):
    evaluations = get_evaluations_from_target_user_id_or_target_type(target_user_id=user_id)
    return calculate_average_scores(evaluations)


def evaluation_for_company():
    evaluations = get_evaluations_from_target_user_id_or_target_type(target_type=1)
    return calculate_average_scores(evaluations)


def calculate_average_scores(evaluations):
    topic_keys = [
        'Company_Fulfillment',
        'Company_Autonomy',
        'Company_GrowthOpportunities',
        'Company_Workload',
        'Company_Stress',
        'Company_WorkLifeBalance',
        'Person_Recognition',
        'Person_Sympathy',
        'Person_Trust',
        'Person_ProSupport',
        'Person_GrowthSupport',
    ]

    average_scores = {}
    for topic in topic_keys:
        total_score = 0
        total_weight = 0
        for evaluation in evaluations:
            score = evaluation.get(topic)
            weight = evaluation.get(f'{topic}Weight')
            if score is not None and weight is not None:
                total_score += score * weight * 100
                total_weight += weight * 100
        average_scores[topic] = total_score / total_weight if total_weight else None

    return average_scores
